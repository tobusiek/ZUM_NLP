{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\manar\\anaconda3\\envs\\bigd\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\manar\\anaconda3\\envs\\bigd\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\manar\\anaconda3\\envs\\bigd\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Requirement already satisfied: anyascii in c:\\users\\manar\\anaconda3\\envs\\bigd\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\manar\\anaconda3\\envs\\bigd\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\manar\\anaconda3\\envs\\bigd\\lib\\site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: click in c:\\users\\manar\\anaconda3\\envs\\bigd\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\manar\\anaconda3\\envs\\bigd\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manar\\anaconda3\\envs\\bigd\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\manar\\anaconda3\\envs\\bigd\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode -q\n",
    "!pip install contractions\n",
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comment</td>\n",
       "      <td>UNCwesRPh</td>\n",
       "      <td>I had been on twitter prior to the musk takeov...</td>\n",
       "      <td>2023-01-30 18:49:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment</td>\n",
       "      <td>wgp3</td>\n",
       "      <td>That article does not say what you imply at al...</td>\n",
       "      <td>2023-01-30 18:49:28+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comment</td>\n",
       "      <td>HighAndDrunk</td>\n",
       "      <td>The OG musk duck lives on my wall.</td>\n",
       "      <td>2023-01-30 18:48:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Louismaxwell23</td>\n",
       "      <td>How dare he speak that way to the great and po...</td>\n",
       "      <td>2023-01-30 18:48:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Copykill</td>\n",
       "      <td>Can’t wait to finally have an excuse not to sh...</td>\n",
       "      <td>2023-01-30 18:48:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85215</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Emble12</td>\n",
       "      <td>Like brain dead piranhas lmao, anything barely...</td>\n",
       "      <td>2023-05-01 07:09:53+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85221</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Pizza_in_Space</td>\n",
       "      <td>What am I lying about? What's my agenda? Pleas...</td>\n",
       "      <td>2023-05-01 07:08:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85223</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Da1realBigA</td>\n",
       "      <td>Hard disagree. I don't think he's a parody of ...</td>\n",
       "      <td>2023-05-01 07:07:36+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85224</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Viperions</td>\n",
       "      <td>Yeah. I think too many things are lining up ri...</td>\n",
       "      <td>2023-05-01 07:07:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85225</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Zealousideal_Plum498</td>\n",
       "      <td>Good choice by Musk. It is impossible to regul...</td>\n",
       "      <td>2023-05-01 07:06:37+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61621 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                author  \\\n",
       "0      Comment             UNCwesRPh   \n",
       "2      Comment                  wgp3   \n",
       "3      Comment          HighAndDrunk   \n",
       "4      Comment        Louismaxwell23   \n",
       "5      Comment              Copykill   \n",
       "...        ...                   ...   \n",
       "85215  Comment               Emble12   \n",
       "85221  Comment        Pizza_in_Space   \n",
       "85223  Comment           Da1realBigA   \n",
       "85224  Comment             Viperions   \n",
       "85225  Comment  Zealousideal_Plum498   \n",
       "\n",
       "                                                    text  \\\n",
       "0      I had been on twitter prior to the musk takeov...   \n",
       "2      That article does not say what you imply at al...   \n",
       "3                     The OG musk duck lives on my wall.   \n",
       "4      How dare he speak that way to the great and po...   \n",
       "5      Can’t wait to finally have an excuse not to sh...   \n",
       "...                                                  ...   \n",
       "85215  Like brain dead piranhas lmao, anything barely...   \n",
       "85221  What am I lying about? What's my agenda? Pleas...   \n",
       "85223  Hard disagree. I don't think he's a parody of ...   \n",
       "85224  Yeah. I think too many things are lining up ri...   \n",
       "85225  Good choice by Musk. It is impossible to regul...   \n",
       "\n",
       "                         created  \n",
       "0      2023-01-30 18:49:31+00:00  \n",
       "2      2023-01-30 18:49:28+00:00  \n",
       "3      2023-01-30 18:48:43+00:00  \n",
       "4      2023-01-30 18:48:26+00:00  \n",
       "5      2023-01-30 18:48:10+00:00  \n",
       "...                          ...  \n",
       "85215  2023-05-01 07:09:53+00:00  \n",
       "85221  2023-05-01 07:08:43+00:00  \n",
       "85223  2023-05-01 07:07:36+00:00  \n",
       "85224  2023-05-01 07:07:15+00:00  \n",
       "85225  2023-05-01 07:06:37+00:00  \n",
       "\n",
       "[61621 rows x 4 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import re\n",
    "import multiprocessing\n",
    "from time import time \n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from unidecode import unidecode\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "df_ = pd.read_csv('musk.csv', index_col=0)\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Comment', '2023-04-30 15:47:51+00:00'], dtype=object)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNCwesRPh</td>\n",
       "      <td>I had been on twitter prior to the musk takeov...</td>\n",
       "      <td>2023-01-30 18:49:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wgp3</td>\n",
       "      <td>That article does not say what you imply at al...</td>\n",
       "      <td>2023-01-30 18:49:28+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HighAndDrunk</td>\n",
       "      <td>The OG musk duck lives on my wall.</td>\n",
       "      <td>2023-01-30 18:48:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Louismaxwell23</td>\n",
       "      <td>How dare he speak that way to the great and po...</td>\n",
       "      <td>2023-01-30 18:48:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Copykill</td>\n",
       "      <td>Can’t wait to finally have an excuse not to sh...</td>\n",
       "      <td>2023-01-30 18:48:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85215</th>\n",
       "      <td>Emble12</td>\n",
       "      <td>Like brain dead piranhas lmao, anything barely...</td>\n",
       "      <td>2023-05-01 07:09:53+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85221</th>\n",
       "      <td>Pizza_in_Space</td>\n",
       "      <td>What am I lying about? What's my agenda? Pleas...</td>\n",
       "      <td>2023-05-01 07:08:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85223</th>\n",
       "      <td>Da1realBigA</td>\n",
       "      <td>Hard disagree. I don't think he's a parody of ...</td>\n",
       "      <td>2023-05-01 07:07:36+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85224</th>\n",
       "      <td>Viperions</td>\n",
       "      <td>Yeah. I think too many things are lining up ri...</td>\n",
       "      <td>2023-05-01 07:07:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85225</th>\n",
       "      <td>Zealousideal_Plum498</td>\n",
       "      <td>Good choice by Musk. It is impossible to regul...</td>\n",
       "      <td>2023-05-01 07:06:37+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61621 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     author  \\\n",
       "0                 UNCwesRPh   \n",
       "2                      wgp3   \n",
       "3              HighAndDrunk   \n",
       "4            Louismaxwell23   \n",
       "5                  Copykill   \n",
       "...                     ...   \n",
       "85215               Emble12   \n",
       "85221        Pizza_in_Space   \n",
       "85223           Da1realBigA   \n",
       "85224             Viperions   \n",
       "85225  Zealousideal_Plum498   \n",
       "\n",
       "                                                    text  \\\n",
       "0      I had been on twitter prior to the musk takeov...   \n",
       "2      That article does not say what you imply at al...   \n",
       "3                     The OG musk duck lives on my wall.   \n",
       "4      How dare he speak that way to the great and po...   \n",
       "5      Can’t wait to finally have an excuse not to sh...   \n",
       "...                                                  ...   \n",
       "85215  Like brain dead piranhas lmao, anything barely...   \n",
       "85221  What am I lying about? What's my agenda? Pleas...   \n",
       "85223  Hard disagree. I don't think he's a parody of ...   \n",
       "85224  Yeah. I think too many things are lining up ri...   \n",
       "85225  Good choice by Musk. It is impossible to regul...   \n",
       "\n",
       "                         created  \n",
       "0      2023-01-30 18:49:31+00:00  \n",
       "2      2023-01-30 18:49:28+00:00  \n",
       "3      2023-01-30 18:48:43+00:00  \n",
       "4      2023-01-30 18:48:26+00:00  \n",
       "5      2023-01-30 18:48:10+00:00  \n",
       "...                          ...  \n",
       "85215  2023-05-01 07:09:53+00:00  \n",
       "85221  2023-05-01 07:08:43+00:00  \n",
       "85223  2023-05-01 07:07:36+00:00  \n",
       "85224  2023-05-01 07:07:15+00:00  \n",
       "85225  2023-05-01 07:06:37+00:00  \n",
       "\n",
       "[61621 rows x 3 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove column with only one value\n",
    "df_.drop(columns=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_and_na(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Remove duplicates and NA values'''\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text:str) -> str:\n",
    "    '''Lower all characters'''\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text:str) -> str:\n",
    "    '''Replace contractions in string of text'''\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text:str) -> str:\n",
    "   '''Remove URLs'''\n",
    "   return re.sub(r\"((www.[^\\s]+)|(https?://[^\\s]+))\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_to_unicode(text:str) -> str:\n",
    "    '''Replace special characters to unicode standard'''\n",
    "    return unidecode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text:str) -> str:\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"'s \", \" is \", text)\n",
    "    text = re.sub(r\"'ve \", \" have \", text)\n",
    "    text = re.sub(r\"n't \", \" not \", text)\n",
    "    text = re.sub(r\"'re \", \" are \", text)\n",
    "    text = re.sub(r\"'d \", \" would \", text)\n",
    "    text = re.sub(r\"'ll \", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"/r/\", \" subreddit \", text)\n",
    "    text = re.sub(r\"0 0\", \"00\", text)\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_.copy()\n",
    "df = remove_duplicates_and_na(df)\n",
    "df['text'] = df['text'].apply(lambda x: lower(x))\n",
    "df['text'] = df['text'].apply(lambda x: remove_url(x))\n",
    "df['text'] = df['text'].apply(lambda x: replace_contractions(x))\n",
    "df['text'] = df['text'].apply(lambda x: replace_to_unicode(x))\n",
    "df['text'] = df['text'].apply(lambda x: text_to_word_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comment</td>\n",
       "      <td>UNCwesRPh</td>\n",
       "      <td>[i, had, been, on, twitter, prior, to, the, mu...</td>\n",
       "      <td>2023-01-30 18:49:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment</td>\n",
       "      <td>wgp3</td>\n",
       "      <td>[that, article, does, not, say, what, you, imp...</td>\n",
       "      <td>2023-01-30 18:49:28+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comment</td>\n",
       "      <td>HighAndDrunk</td>\n",
       "      <td>[the, og, musk, duck, lives, on, my, wall]</td>\n",
       "      <td>2023-01-30 18:48:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Louismaxwell23</td>\n",
       "      <td>[how, dare, he, speak, that, way, to, the, gre...</td>\n",
       "      <td>2023-01-30 18:48:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Copykill</td>\n",
       "      <td>[cannot, wait, to, finally, have, an, excuse, ...</td>\n",
       "      <td>2023-01-30 18:48:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85215</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Emble12</td>\n",
       "      <td>[like, brain, dead, piranhas, lmao, anything, ...</td>\n",
       "      <td>2023-05-01 07:09:53+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85221</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Pizza_in_Space</td>\n",
       "      <td>[what, am, i, lying, about, what, is, my, agen...</td>\n",
       "      <td>2023-05-01 07:08:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85223</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Da1realBigA</td>\n",
       "      <td>[hard, disagree, i, do, not, think, he, is, a,...</td>\n",
       "      <td>2023-05-01 07:07:36+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85224</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Viperions</td>\n",
       "      <td>[yeah, i, think, too, many, things, are, linin...</td>\n",
       "      <td>2023-05-01 07:07:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85225</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Zealousideal_Plum498</td>\n",
       "      <td>[good, choice, by, musk, it, is, impossible, t...</td>\n",
       "      <td>2023-05-01 07:06:37+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61619 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                author  \\\n",
       "0      Comment             UNCwesRPh   \n",
       "2      Comment                  wgp3   \n",
       "3      Comment          HighAndDrunk   \n",
       "4      Comment        Louismaxwell23   \n",
       "5      Comment              Copykill   \n",
       "...        ...                   ...   \n",
       "85215  Comment               Emble12   \n",
       "85221  Comment        Pizza_in_Space   \n",
       "85223  Comment           Da1realBigA   \n",
       "85224  Comment             Viperions   \n",
       "85225  Comment  Zealousideal_Plum498   \n",
       "\n",
       "                                                    text  \\\n",
       "0      [i, had, been, on, twitter, prior, to, the, mu...   \n",
       "2      [that, article, does, not, say, what, you, imp...   \n",
       "3             [the, og, musk, duck, lives, on, my, wall]   \n",
       "4      [how, dare, he, speak, that, way, to, the, gre...   \n",
       "5      [cannot, wait, to, finally, have, an, excuse, ...   \n",
       "...                                                  ...   \n",
       "85215  [like, brain, dead, piranhas, lmao, anything, ...   \n",
       "85221  [what, am, i, lying, about, what, is, my, agen...   \n",
       "85223  [hard, disagree, i, do, not, think, he, is, a,...   \n",
       "85224  [yeah, i, think, too, many, things, are, linin...   \n",
       "85225  [good, choice, by, musk, it, is, impossible, t...   \n",
       "\n",
       "                         created  \n",
       "0      2023-01-30 18:49:31+00:00  \n",
       "2      2023-01-30 18:49:28+00:00  \n",
       "3      2023-01-30 18:48:43+00:00  \n",
       "4      2023-01-30 18:48:26+00:00  \n",
       "5      2023-01-30 18:48:10+00:00  \n",
       "...                          ...  \n",
       "85215  2023-05-01 07:09:53+00:00  \n",
       "85221  2023-05-01 07:08:43+00:00  \n",
       "85223  2023-05-01 07:07:36+00:00  \n",
       "85224  2023-05-01 07:07:15+00:00  \n",
       "85225  2023-05-01 07:06:37+00:00  \n",
       "\n",
       "[61619 rows x 4 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Remove stop words'''\n",
    "    stops = set(stopwords.words('english'))\n",
    "    df['text'] = df['text'].apply(lambda x: [item for item in x if item not in stops])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = remove_stop_words(df)\n",
    "df = df[df.text.str.len() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:45:58: collecting all words and their counts\n",
      "INFO - 15:45:58: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 15:46:00: PROGRESS: at sentence #50000, processed 1970668 words and 1235972 word types\n",
      "INFO - 15:46:00: collected 1463211 token types (unigram + bigrams) from a corpus of 2431210 words and 61282 sentences\n",
      "INFO - 15:46:00: merged Phrases<1463211 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 15:46:00: Phrases lifecycle event {'msg': 'built Phrases<1463211 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000> in 2.00s', 'datetime': '2023-05-20T15:46:00.847714', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'created'}\n",
      "INFO - 15:46:00: exporting phrases from Phrases<1463211 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 15:46:02: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<114755 phrases, min_count=1, threshold=10.0> from Phrases<1463211 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000> in 1.73s', 'datetime': '2023-05-20T15:46:02.601819', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['article',\n",
       " 'say',\n",
       " 'imply',\n",
       " 'states',\n",
       " 'feature',\n",
       " 'turns',\n",
       " 'crashes',\n",
       " 'tesla',\n",
       " 'admits',\n",
       " 'default_behavior',\n",
       " 'never',\n",
       " 'tried_hide',\n",
       " 'hence',\n",
       " 'whole',\n",
       " 'counting',\n",
       " 'crash',\n",
       " 'disengaged',\n",
       " 'within',\n",
       " '5_seconds',\n",
       " 'crash',\n",
       " 'article',\n",
       " 'say',\n",
       " 'brings',\n",
       " 'question',\n",
       " 'times',\n",
       " 'musk',\n",
       " 'said',\n",
       " 'feature',\n",
       " 'give',\n",
       " 'proof',\n",
       " 'times',\n",
       " 'musk',\n",
       " 'said',\n",
       " 'fact',\n",
       " 'referring',\n",
       " 'crashes',\n",
       " 'turned',\n",
       " 'right',\n",
       " 'crash',\n",
       " 'speculation',\n",
       " 'best']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = [row for row in df.text]\n",
    "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61282, 4)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('musk_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:46:03: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2023-05-20T15:46:03.189513', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'created'}\n",
      "INFO - 15:46:03: collecting all words and their counts\n",
      "INFO - 15:46:03: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 15:46:04: PROGRESS: at sentence #50000, processed 1628170 words, keeping 175838 word types\n",
      "INFO - 15:46:04: collected 193416 word types from a corpus of 2009180 raw words and 61282 sentences\n",
      "INFO - 15:46:04: Creating a fresh vocabulary\n",
      "INFO - 15:46:04: Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 70944 unique words (36.68% of original 193416, drops 122472)', 'datetime': '2023-05-20T15:46:04.886599', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 15:46:04: Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 1829735 word corpus (91.07% of original 2009180, drops 179445)', 'datetime': '2023-05-20T15:46:04.887090', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 15:46:05: deleting the raw counts dictionary of 193416 items\n",
      "INFO - 15:46:05: sample=1e-05 downsamples 4582 most-common words\n",
      "INFO - 15:46:05: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 858669.6628684478 word corpus (46.9%% of prior 1829735)', 'datetime': '2023-05-20T15:46:05.071927', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 15:46:05: estimated required memory for 70944 words and 300 dimensions: 205737600 bytes\n",
      "INFO - 15:46:05: resetting layer weights\n",
      "INFO - 15:46:05: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-05-20T15:46:05.447713', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.04 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     vector_size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:46:05: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 70944 vocabulary and 300 features, using sg=0 hs=0 sample=1e-05 negative=20 window=4 shrink_windows=True', 'datetime': '2023-05-20T15:46:05.471098', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'train'}\n",
      "INFO - 15:46:06: EPOCH 0 - PROGRESS: at 31.36% examples, 269543 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 15:46:07: EPOCH 0 - PROGRESS: at 66.60% examples, 285040 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:08: EPOCH 0: training on 2009180 raw words (858306 effective words) took 2.9s, 298831 effective words/s\n",
      "INFO - 15:46:09: EPOCH 1 - PROGRESS: at 32.88% examples, 280976 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:10: EPOCH 1 - PROGRESS: at 69.13% examples, 295724 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:11: EPOCH 1: training on 2009180 raw words (858758 effective words) took 2.7s, 316315 effective words/s\n",
      "INFO - 15:46:12: EPOCH 2 - PROGRESS: at 32.96% examples, 280543 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 15:46:13: EPOCH 2 - PROGRESS: at 58.73% examples, 251053 words/s, in_qsize 13, out_qsize 1\n",
      "INFO - 15:46:14: EPOCH 2: training on 2009180 raw words (858192 effective words) took 3.0s, 282833 effective words/s\n",
      "INFO - 15:46:15: EPOCH 3 - PROGRESS: at 31.84% examples, 259554 words/s, in_qsize 9, out_qsize 6\n",
      "INFO - 15:46:16: EPOCH 3 - PROGRESS: at 72.77% examples, 301908 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:16: EPOCH 3: training on 2009180 raw words (858603 effective words) took 2.7s, 315554 effective words/s\n",
      "INFO - 15:46:17: EPOCH 4 - PROGRESS: at 33.89% examples, 282868 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:18: EPOCH 4 - PROGRESS: at 72.31% examples, 301510 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:19: EPOCH 4: training on 2009180 raw words (859451 effective words) took 2.7s, 317339 effective words/s\n",
      "INFO - 15:46:20: EPOCH 5 - PROGRESS: at 32.41% examples, 279574 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:21: EPOCH 5 - PROGRESS: at 69.69% examples, 292873 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 15:46:22: EPOCH 5: training on 2009180 raw words (858186 effective words) took 2.7s, 313884 effective words/s\n",
      "INFO - 15:46:23: EPOCH 6 - PROGRESS: at 32.64% examples, 279249 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:24: EPOCH 6 - PROGRESS: at 70.24% examples, 294891 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:25: EPOCH 6: training on 2009180 raw words (859152 effective words) took 2.7s, 315387 effective words/s\n",
      "INFO - 15:46:26: EPOCH 7 - PROGRESS: at 33.95% examples, 291528 words/s, in_qsize 11, out_qsize 1\n",
      "INFO - 15:46:27: EPOCH 7 - PROGRESS: at 70.25% examples, 296577 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:27: EPOCH 7: training on 2009180 raw words (858621 effective words) took 2.7s, 316359 effective words/s\n",
      "INFO - 15:46:28: EPOCH 8 - PROGRESS: at 31.36% examples, 272405 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 15:46:29: EPOCH 8 - PROGRESS: at 67.55% examples, 285812 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:30: EPOCH 8: training on 2009180 raw words (858583 effective words) took 2.8s, 309082 effective words/s\n",
      "INFO - 15:46:31: EPOCH 9 - PROGRESS: at 32.41% examples, 261478 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 15:46:32: EPOCH 9 - PROGRESS: at 69.69% examples, 283195 words/s, in_qsize 10, out_qsize 5\n",
      "INFO - 15:46:33: EPOCH 9: training on 2009180 raw words (857980 effective words) took 2.8s, 307589 effective words/s\n",
      "INFO - 15:46:34: EPOCH 10 - PROGRESS: at 28.71% examples, 242995 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 15:46:35: EPOCH 10 - PROGRESS: at 64.23% examples, 273912 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 15:46:36: EPOCH 10: training on 2009180 raw words (857971 effective words) took 2.9s, 293096 effective words/s\n",
      "INFO - 15:46:37: EPOCH 11 - PROGRESS: at 23.78% examples, 200690 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 15:46:38: EPOCH 11 - PROGRESS: at 58.90% examples, 247259 words/s, in_qsize 12, out_qsize 2\n",
      "INFO - 15:46:39: EPOCH 11: training on 2009180 raw words (858994 effective words) took 3.0s, 282425 effective words/s\n",
      "INFO - 15:46:40: EPOCH 12 - PROGRESS: at 31.92% examples, 275616 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:41: EPOCH 12 - PROGRESS: at 68.05% examples, 287912 words/s, in_qsize 13, out_qsize 1\n",
      "INFO - 15:46:42: EPOCH 12: training on 2009180 raw words (858612 effective words) took 2.8s, 308879 effective words/s\n",
      "INFO - 15:46:43: EPOCH 13 - PROGRESS: at 30.57% examples, 255773 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:44: EPOCH 13 - PROGRESS: at 67.56% examples, 280059 words/s, in_qsize 10, out_qsize 3\n",
      "INFO - 15:46:45: EPOCH 13: training on 2009180 raw words (858527 effective words) took 2.8s, 302379 effective words/s\n",
      "INFO - 15:46:46: EPOCH 14 - PROGRESS: at 30.20% examples, 255489 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 15:46:47: EPOCH 14 - PROGRESS: at 66.11% examples, 280145 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 15:46:47: EPOCH 14: training on 2009180 raw words (859301 effective words) took 2.8s, 303191 effective words/s\n",
      "INFO - 15:46:48: EPOCH 15 - PROGRESS: at 31.07% examples, 265270 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 15:46:49: EPOCH 15 - PROGRESS: at 66.16% examples, 282607 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 15:46:50: EPOCH 15: training on 2009180 raw words (858892 effective words) took 2.9s, 300791 effective words/s\n",
      "INFO - 15:46:51: EPOCH 16 - PROGRESS: at 30.96% examples, 264937 words/s, in_qsize 10, out_qsize 2\n",
      "INFO - 15:46:52: EPOCH 16 - PROGRESS: at 66.15% examples, 278347 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:46:53: EPOCH 16: training on 2009180 raw words (858180 effective words) took 2.9s, 301090 effective words/s\n",
      "INFO - 15:46:54: EPOCH 17 - PROGRESS: at 28.58% examples, 241935 words/s, in_qsize 13, out_qsize 3\n",
      "INFO - 15:46:55: EPOCH 17 - PROGRESS: at 66.60% examples, 277002 words/s, in_qsize 11, out_qsize 4\n",
      "INFO - 15:46:56: EPOCH 17: training on 2009180 raw words (858653 effective words) took 2.8s, 303708 effective words/s\n",
      "INFO - 15:46:57: EPOCH 18 - PROGRESS: at 29.68% examples, 253266 words/s, in_qsize 10, out_qsize 4\n",
      "INFO - 15:46:58: EPOCH 18 - PROGRESS: at 63.71% examples, 269970 words/s, in_qsize 11, out_qsize 2\n",
      "INFO - 15:46:59: EPOCH 18: training on 2009180 raw words (859110 effective words) took 3.0s, 286478 effective words/s\n",
      "INFO - 15:47:00: EPOCH 19 - PROGRESS: at 28.23% examples, 235671 words/s, in_qsize 11, out_qsize 0\n",
      "INFO - 15:47:01: EPOCH 19 - PROGRESS: at 60.34% examples, 250588 words/s, in_qsize 10, out_qsize 4\n",
      "INFO - 15:47:02: EPOCH 19 - PROGRESS: at 98.24% examples, 271538 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 15:47:02: EPOCH 19: training on 2009180 raw words (858779 effective words) took 3.1s, 276237 effective words/s\n",
      "INFO - 15:47:03: EPOCH 20 - PROGRESS: at 29.55% examples, 245425 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:47:04: EPOCH 20 - PROGRESS: at 57.30% examples, 234342 words/s, in_qsize 11, out_qsize 2\n",
      "INFO - 15:47:05: EPOCH 20 - PROGRESS: at 93.95% examples, 258055 words/s, in_qsize 11, out_qsize 2\n",
      "INFO - 15:47:05: EPOCH 20: training on 2009180 raw words (858484 effective words) took 3.2s, 267778 effective words/s\n",
      "INFO - 15:47:06: EPOCH 21 - PROGRESS: at 27.74% examples, 233414 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:47:07: EPOCH 21 - PROGRESS: at 59.28% examples, 254455 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:47:08: EPOCH 21 - PROGRESS: at 95.45% examples, 269306 words/s, in_qsize 11, out_qsize 0\n",
      "INFO - 15:47:08: EPOCH 21: training on 2009180 raw words (859003 effective words) took 3.1s, 275929 effective words/s\n",
      "INFO - 15:47:09: EPOCH 22 - PROGRESS: at 28.06% examples, 240632 words/s, in_qsize 10, out_qsize 4\n",
      "INFO - 15:47:10: EPOCH 22 - PROGRESS: at 58.73% examples, 250106 words/s, in_qsize 11, out_qsize 3\n",
      "INFO - 15:47:11: EPOCH 22 - PROGRESS: at 91.89% examples, 258602 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 15:47:12: EPOCH 22: training on 2009180 raw words (858891 effective words) took 3.2s, 272226 effective words/s\n",
      "INFO - 15:47:13: EPOCH 23 - PROGRESS: at 29.12% examples, 245533 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:47:14: EPOCH 23 - PROGRESS: at 61.15% examples, 259930 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 15:47:15: EPOCH 23 - PROGRESS: at 98.25% examples, 277571 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 15:47:15: EPOCH 23: training on 2009180 raw words (859235 effective words) took 3.1s, 280744 effective words/s\n",
      "INFO - 15:47:16: EPOCH 24 - PROGRESS: at 30.53% examples, 247874 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 15:47:17: EPOCH 24 - PROGRESS: at 65.60% examples, 270891 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:47:18: EPOCH 24: training on 2009180 raw words (858769 effective words) took 2.9s, 292538 effective words/s\n",
      "INFO - 15:47:19: EPOCH 25 - PROGRESS: at 28.76% examples, 244858 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 15:47:20: EPOCH 25 - PROGRESS: at 65.16% examples, 272884 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 15:47:21: EPOCH 25: training on 2009180 raw words (859343 effective words) took 2.9s, 293008 effective words/s\n",
      "INFO - 15:47:22: EPOCH 26 - PROGRESS: at 26.73% examples, 227751 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 15:47:23: EPOCH 26 - PROGRESS: at 59.77% examples, 256734 words/s, in_qsize 13, out_qsize 1\n",
      "INFO - 15:47:24: EPOCH 26 - PROGRESS: at 99.46% examples, 282001 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 15:47:24: EPOCH 26: training on 2009180 raw words (858301 effective words) took 3.0s, 283080 effective words/s\n",
      "INFO - 15:47:25: EPOCH 27 - PROGRESS: at 30.00% examples, 251802 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 15:47:26: EPOCH 27 - PROGRESS: at 63.26% examples, 266960 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:47:27: EPOCH 27: training on 2009180 raw words (858852 effective words) took 3.0s, 288940 effective words/s\n",
      "INFO - 15:47:28: EPOCH 28 - PROGRESS: at 29.06% examples, 245760 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 15:47:29: EPOCH 28 - PROGRESS: at 63.26% examples, 267701 words/s, in_qsize 9, out_qsize 6\n",
      "INFO - 15:47:30: EPOCH 28 - PROGRESS: at 90.22% examples, 249236 words/s, in_qsize 14, out_qsize 1\n",
      "INFO - 15:47:30: EPOCH 28: training on 2009180 raw words (858377 effective words) took 3.3s, 261071 effective words/s\n",
      "INFO - 15:47:31: EPOCH 29 - PROGRESS: at 25.70% examples, 217191 words/s, in_qsize 10, out_qsize 4\n",
      "INFO - 15:47:32: EPOCH 29 - PROGRESS: at 59.77% examples, 255090 words/s, in_qsize 13, out_qsize 1\n",
      "INFO - 15:47:33: EPOCH 29 - PROGRESS: at 91.51% examples, 256489 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 15:47:33: EPOCH 29: training on 2009180 raw words (858446 effective words) took 3.2s, 265966 effective words/s\n",
      "INFO - 15:47:33: Word2Vec lifecycle event {'msg': 'training on 60275400 raw words (25760552 effective words) took 88.1s, 292448 effective words/s', 'datetime': '2023-05-20T15:47:33.556202', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'train'}\n",
      "/var/folders/dn/6362h3w12mqfb_jh4vg90vp40000gn/T/ipykernel_44609/1733578886.py:10: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n",
      "WARNING - 15:47:33: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 1.47 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "w2v_model.train(sentences,\n",
    "                total_examples=w2v_model.corpus_count,\n",
    "                epochs=30,\n",
    "                report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:47:33: Word2Vec lifecycle event {'fname_or_handle': 'word2vec.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-05-20T15:47:33.636073', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'saving'}\n",
      "INFO - 15:47:33: storing np array 'vectors' to word2vec.model.wv.vectors.npy\n",
      "INFO - 15:47:33: storing np array 'syn1neg' to word2vec.model.syn1neg.npy\n",
      "INFO - 15:47:33: not storing attribute cum_table\n",
      "INFO - 15:47:33: saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:48:00: loading Word2Vec object from word2vec.model\n",
      "INFO - 15:48:01: loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "INFO - 15:48:01: loading vectors from word2vec.model.wv.vectors.npy with mmap=None\n",
      "INFO - 15:48:01: loading syn1neg from word2vec.model.syn1neg.npy with mmap=None\n",
      "INFO - 15:48:01: setting ignored attribute cum_table to None\n",
      "INFO - 15:48:01: Word2Vec lifecycle event {'fname': 'word2vec.model', 'datetime': '2023-05-20T15:48:01.365443', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'loaded'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(max_iter=1000, n_clusters=2, n_init=50, random_state=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(max_iter=1000, n_clusters=2, n_init=50, random_state=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(max_iter=1000, n_clusters=2, n_init=50, random_state=True)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "word_vectors = Word2Vec.load(\"word2vec.model\").wv\n",
    "\n",
    "model = KMeans(n_clusters=2,\n",
    "               max_iter=1000,\n",
    "               random_state=True,\n",
    "               n_init=50)\n",
    "model.fit(X=word_vectors.vectors.astype('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parma_violet', 0.9978695511817932),\n",
       " ('needles_moment', 0.997712254524231),\n",
       " ('20_ringing', 0.9977063536643982),\n",
       " ('sugar_absinthe', 0.9971377849578857),\n",
       " ('candy_wood', 0.9970799088478088),\n",
       " ('booked_capital', 0.9970647692680359),\n",
       " ('adds_dimension', 0.996945321559906),\n",
       " ('tortoiseshell_glasses', 0.996891975402832),\n",
       " ('sawed_branch', 0.996888279914856),\n",
       " ('carrot_ambrette', 0.9968278408050537)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_vector(model.cluster_centers_[1], topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cluster_index = 1\n",
    "positive_cluster_center = model.cluster_centers_[positive_cluster_index]\n",
    "negative_cluster_center = model.cluster_centers_[1-positive_cluster_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_vectors.index_to_key)\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])\n",
    "words['cluster_value'] = [1 if i==positive_cluster_index else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>musk</td>\n",
       "      <td>[-0.018240415, 0.11474369, 0.06485452, 0.06379...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.985885</td>\n",
       "      <td>-0.985885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>[-0.07806011, 0.0885233, 0.023241187, -0.01320...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.841872</td>\n",
       "      <td>0.841872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elon_musk</td>\n",
       "      <td>[0.092020035, 0.039484542, 0.04520107, 0.04053...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.078684</td>\n",
       "      <td>-1.078684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people</td>\n",
       "      <td>[0.0500488, 0.090776466, -0.027416097, 0.05665...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.372970</td>\n",
       "      <td>-1.372970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>like</td>\n",
       "      <td>[0.031427264, 0.14492418, 0.018667255, 0.05746...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.332847</td>\n",
       "      <td>-1.332847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>would</td>\n",
       "      <td>[0.017006392, 0.089235775, 0.04305824, 0.11443...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.397296</td>\n",
       "      <td>-1.397296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>twitter</td>\n",
       "      <td>[-0.017191386, 0.037429247, 0.07227137, 0.0013...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.055537</td>\n",
       "      <td>-1.055537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>:</td>\n",
       "      <td>[0.12599944, -0.012939106, 0.060281314, -0.044...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.907072</td>\n",
       "      <td>0.907072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>one</td>\n",
       "      <td>[0.055811036, 0.07972116, 0.03603393, 0.051790...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.392335</td>\n",
       "      <td>-1.392335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>think</td>\n",
       "      <td>[0.035479654, 0.103134885, -0.0034316035, 0.06...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.438220</td>\n",
       "      <td>-1.438220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       words                                            vectors  cluster  \\\n",
       "0       musk  [-0.018240415, 0.11474369, 0.06485452, 0.06379...        0   \n",
       "1          -  [-0.07806011, 0.0885233, 0.023241187, -0.01320...        1   \n",
       "2  elon_musk  [0.092020035, 0.039484542, 0.04520107, 0.04053...        0   \n",
       "3     people  [0.0500488, 0.090776466, -0.027416097, 0.05665...        0   \n",
       "4       like  [0.031427264, 0.14492418, 0.018667255, 0.05746...        0   \n",
       "5      would  [0.017006392, 0.089235775, 0.04305824, 0.11443...        0   \n",
       "6    twitter  [-0.017191386, 0.037429247, 0.07227137, 0.0013...        0   \n",
       "7          :  [0.12599944, -0.012939106, 0.060281314, -0.044...        1   \n",
       "8        one  [0.055811036, 0.07972116, 0.03603393, 0.051790...        0   \n",
       "9      think  [0.035479654, 0.103134885, -0.0034316035, 0.06...        0   \n",
       "\n",
       "   cluster_value  closeness_score  sentiment_coeff  \n",
       "0             -1         0.985885        -0.985885  \n",
       "1              1         0.841872         0.841872  \n",
       "2             -1         1.078684        -1.078684  \n",
       "3             -1         1.372970        -1.372970  \n",
       "4             -1         1.332847        -1.332847  \n",
       "5             -1         1.397296        -1.397296  \n",
       "6             -1         1.055537        -1.055537  \n",
       "7              1         0.907072         0.907072  \n",
       "8             -1         1.392335        -1.392335  \n",
       "9             -1         1.438220        -1.438220  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
