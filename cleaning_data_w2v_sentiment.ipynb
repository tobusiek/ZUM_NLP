{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unidecode -q\n",
    "!pip install contractions\n",
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comment</td>\n",
       "      <td>UNCwesRPh</td>\n",
       "      <td>I had been on twitter prior to the musk takeov...</td>\n",
       "      <td>2023-01-30 18:49:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment</td>\n",
       "      <td>wgp3</td>\n",
       "      <td>That article does not say what you imply at al...</td>\n",
       "      <td>2023-01-30 18:49:28+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comment</td>\n",
       "      <td>HighAndDrunk</td>\n",
       "      <td>The OG musk duck lives on my wall.</td>\n",
       "      <td>2023-01-30 18:48:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Louismaxwell23</td>\n",
       "      <td>How dare he speak that way to the great and po...</td>\n",
       "      <td>2023-01-30 18:48:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Copykill</td>\n",
       "      <td>Can’t wait to finally have an excuse not to sh...</td>\n",
       "      <td>2023-01-30 18:48:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85215</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Emble12</td>\n",
       "      <td>Like brain dead piranhas lmao, anything barely...</td>\n",
       "      <td>2023-05-01 07:09:53+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85221</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Pizza_in_Space</td>\n",
       "      <td>What am I lying about? What's my agenda? Pleas...</td>\n",
       "      <td>2023-05-01 07:08:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85223</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Da1realBigA</td>\n",
       "      <td>Hard disagree. I don't think he's a parody of ...</td>\n",
       "      <td>2023-05-01 07:07:36+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85224</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Viperions</td>\n",
       "      <td>Yeah. I think too many things are lining up ri...</td>\n",
       "      <td>2023-05-01 07:07:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85225</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Zealousideal_Plum498</td>\n",
       "      <td>Good choice by Musk. It is impossible to regul...</td>\n",
       "      <td>2023-05-01 07:06:37+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61621 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                author  \\\n",
       "0      Comment             UNCwesRPh   \n",
       "2      Comment                  wgp3   \n",
       "3      Comment          HighAndDrunk   \n",
       "4      Comment        Louismaxwell23   \n",
       "5      Comment              Copykill   \n",
       "...        ...                   ...   \n",
       "85215  Comment               Emble12   \n",
       "85221  Comment        Pizza_in_Space   \n",
       "85223  Comment           Da1realBigA   \n",
       "85224  Comment             Viperions   \n",
       "85225  Comment  Zealousideal_Plum498   \n",
       "\n",
       "                                                    text  \\\n",
       "0      I had been on twitter prior to the musk takeov...   \n",
       "2      That article does not say what you imply at al...   \n",
       "3                     The OG musk duck lives on my wall.   \n",
       "4      How dare he speak that way to the great and po...   \n",
       "5      Can’t wait to finally have an excuse not to sh...   \n",
       "...                                                  ...   \n",
       "85215  Like brain dead piranhas lmao, anything barely...   \n",
       "85221  What am I lying about? What's my agenda? Pleas...   \n",
       "85223  Hard disagree. I don't think he's a parody of ...   \n",
       "85224  Yeah. I think too many things are lining up ri...   \n",
       "85225  Good choice by Musk. It is impossible to regul...   \n",
       "\n",
       "                         created  \n",
       "0      2023-01-30 18:49:31+00:00  \n",
       "2      2023-01-30 18:49:28+00:00  \n",
       "3      2023-01-30 18:48:43+00:00  \n",
       "4      2023-01-30 18:48:26+00:00  \n",
       "5      2023-01-30 18:48:10+00:00  \n",
       "...                          ...  \n",
       "85215  2023-05-01 07:09:53+00:00  \n",
       "85221  2023-05-01 07:08:43+00:00  \n",
       "85223  2023-05-01 07:07:36+00:00  \n",
       "85224  2023-05-01 07:07:15+00:00  \n",
       "85225  2023-05-01 07:06:37+00:00  \n",
       "\n",
       "[61621 rows x 4 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import re\n",
    "import multiprocessing\n",
    "from time import time \n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from unidecode import unidecode\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "df_ = pd.read_csv('musk.csv', index_col=0)\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Comment', '2023-04-30 15:47:51+00:00'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNCwesRPh</td>\n",
       "      <td>I had been on twitter prior to the musk takeov...</td>\n",
       "      <td>2023-01-30 18:49:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wgp3</td>\n",
       "      <td>That article does not say what you imply at al...</td>\n",
       "      <td>2023-01-30 18:49:28+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HighAndDrunk</td>\n",
       "      <td>The OG musk duck lives on my wall.</td>\n",
       "      <td>2023-01-30 18:48:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Louismaxwell23</td>\n",
       "      <td>How dare he speak that way to the great and po...</td>\n",
       "      <td>2023-01-30 18:48:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Copykill</td>\n",
       "      <td>Can’t wait to finally have an excuse not to sh...</td>\n",
       "      <td>2023-01-30 18:48:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85215</th>\n",
       "      <td>Emble12</td>\n",
       "      <td>Like brain dead piranhas lmao, anything barely...</td>\n",
       "      <td>2023-05-01 07:09:53+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85221</th>\n",
       "      <td>Pizza_in_Space</td>\n",
       "      <td>What am I lying about? What's my agenda? Pleas...</td>\n",
       "      <td>2023-05-01 07:08:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85223</th>\n",
       "      <td>Da1realBigA</td>\n",
       "      <td>Hard disagree. I don't think he's a parody of ...</td>\n",
       "      <td>2023-05-01 07:07:36+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85224</th>\n",
       "      <td>Viperions</td>\n",
       "      <td>Yeah. I think too many things are lining up ri...</td>\n",
       "      <td>2023-05-01 07:07:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85225</th>\n",
       "      <td>Zealousideal_Plum498</td>\n",
       "      <td>Good choice by Musk. It is impossible to regul...</td>\n",
       "      <td>2023-05-01 07:06:37+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61621 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     author  \\\n",
       "0                 UNCwesRPh   \n",
       "2                      wgp3   \n",
       "3              HighAndDrunk   \n",
       "4            Louismaxwell23   \n",
       "5                  Copykill   \n",
       "...                     ...   \n",
       "85215               Emble12   \n",
       "85221        Pizza_in_Space   \n",
       "85223           Da1realBigA   \n",
       "85224             Viperions   \n",
       "85225  Zealousideal_Plum498   \n",
       "\n",
       "                                                    text  \\\n",
       "0      I had been on twitter prior to the musk takeov...   \n",
       "2      That article does not say what you imply at al...   \n",
       "3                     The OG musk duck lives on my wall.   \n",
       "4      How dare he speak that way to the great and po...   \n",
       "5      Can’t wait to finally have an excuse not to sh...   \n",
       "...                                                  ...   \n",
       "85215  Like brain dead piranhas lmao, anything barely...   \n",
       "85221  What am I lying about? What's my agenda? Pleas...   \n",
       "85223  Hard disagree. I don't think he's a parody of ...   \n",
       "85224  Yeah. I think too many things are lining up ri...   \n",
       "85225  Good choice by Musk. It is impossible to regul...   \n",
       "\n",
       "                         created  \n",
       "0      2023-01-30 18:49:31+00:00  \n",
       "2      2023-01-30 18:49:28+00:00  \n",
       "3      2023-01-30 18:48:43+00:00  \n",
       "4      2023-01-30 18:48:26+00:00  \n",
       "5      2023-01-30 18:48:10+00:00  \n",
       "...                          ...  \n",
       "85215  2023-05-01 07:09:53+00:00  \n",
       "85221  2023-05-01 07:08:43+00:00  \n",
       "85223  2023-05-01 07:07:36+00:00  \n",
       "85224  2023-05-01 07:07:15+00:00  \n",
       "85225  2023-05-01 07:06:37+00:00  \n",
       "\n",
       "[61621 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove column with only one value\n",
    "df_.drop(columns=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_and_na(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Remove duplicates and NA values'''\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text:str) -> str:\n",
    "    '''Lower all characters'''\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text:str) -> str:\n",
    "    '''Replace contractions in string of text'''\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text:str) -> str:\n",
    "   '''Remove URLs'''\n",
    "   return re.sub(r\"((www.[^\\s]+)|(https?://[^\\s]+))\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_to_unicode(text:str) -> str:\n",
    "    '''Replace special characters to unicode standard'''\n",
    "    return unidecode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text:str) -> str:\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"'s \", \" is \", text)\n",
    "    text = re.sub(r\"'ve \", \" have \", text)\n",
    "    text = re.sub(r\"n't \", \" not \", text)\n",
    "    text = re.sub(r\"'re \", \" are \", text)\n",
    "    text = re.sub(r\"'d \", \" would \", text)\n",
    "    text = re.sub(r\"'ll \", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"/r/\", \" subreddit \", text)\n",
    "    text = re.sub(r\"0 0\", \"00\", text)\n",
    "    text = re.sub(r\";\", \" ; \", text)\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_.copy()\n",
    "df = remove_duplicates_and_na(df)\n",
    "df['text'] = df['text'].apply(lambda x: lower(x))\n",
    "df['text'] = df['text'].apply(lambda x: remove_url(x))\n",
    "df['text'] = df['text'].apply(lambda x: replace_contractions(x))\n",
    "df['text'] = df['text'].apply(lambda x: replace_to_unicode(x))\n",
    "df['text'] = df['text'].apply(lambda x: lower(x))\n",
    "df['text'] = df['text'].apply(lambda x: text_to_word_list(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comment</td>\n",
       "      <td>UNCwesRPh</td>\n",
       "      <td>[i, had, been, on, twitter, prior, to, the, mu...</td>\n",
       "      <td>2023-01-30 18:49:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment</td>\n",
       "      <td>wgp3</td>\n",
       "      <td>[that, article, does, not, say, what, you, imp...</td>\n",
       "      <td>2023-01-30 18:49:28+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comment</td>\n",
       "      <td>HighAndDrunk</td>\n",
       "      <td>[the, og, musk, duck, lives, on, my, wall]</td>\n",
       "      <td>2023-01-30 18:48:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Louismaxwell23</td>\n",
       "      <td>[how, dare, he, speak, that, way, to, the, gre...</td>\n",
       "      <td>2023-01-30 18:48:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Copykill</td>\n",
       "      <td>[cannot, wait, to, finally, have, an, excuse, ...</td>\n",
       "      <td>2023-01-30 18:48:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85215</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Emble12</td>\n",
       "      <td>[like, brain, dead, piranhas, lmao, anything, ...</td>\n",
       "      <td>2023-05-01 07:09:53+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85221</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Pizza_in_Space</td>\n",
       "      <td>[what, am, i, lying, about, what, is, my, agen...</td>\n",
       "      <td>2023-05-01 07:08:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85223</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Da1realBigA</td>\n",
       "      <td>[hard, disagree, i, do, not, think, he, is, a,...</td>\n",
       "      <td>2023-05-01 07:07:36+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85224</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Viperions</td>\n",
       "      <td>[yeah, i, think, too, many, things, are, linin...</td>\n",
       "      <td>2023-05-01 07:07:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85225</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Zealousideal_Plum498</td>\n",
       "      <td>[good, choice, by, musk, it, is, impossible, t...</td>\n",
       "      <td>2023-05-01 07:06:37+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61619 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                author  \\\n",
       "0      Comment             UNCwesRPh   \n",
       "2      Comment                  wgp3   \n",
       "3      Comment          HighAndDrunk   \n",
       "4      Comment        Louismaxwell23   \n",
       "5      Comment              Copykill   \n",
       "...        ...                   ...   \n",
       "85215  Comment               Emble12   \n",
       "85221  Comment        Pizza_in_Space   \n",
       "85223  Comment           Da1realBigA   \n",
       "85224  Comment             Viperions   \n",
       "85225  Comment  Zealousideal_Plum498   \n",
       "\n",
       "                                                    text  \\\n",
       "0      [i, had, been, on, twitter, prior, to, the, mu...   \n",
       "2      [that, article, does, not, say, what, you, imp...   \n",
       "3             [the, og, musk, duck, lives, on, my, wall]   \n",
       "4      [how, dare, he, speak, that, way, to, the, gre...   \n",
       "5      [cannot, wait, to, finally, have, an, excuse, ...   \n",
       "...                                                  ...   \n",
       "85215  [like, brain, dead, piranhas, lmao, anything, ...   \n",
       "85221  [what, am, i, lying, about, what, is, my, agen...   \n",
       "85223  [hard, disagree, i, do, not, think, he, is, a,...   \n",
       "85224  [yeah, i, think, too, many, things, are, linin...   \n",
       "85225  [good, choice, by, musk, it, is, impossible, t...   \n",
       "\n",
       "                         created  \n",
       "0      2023-01-30 18:49:31+00:00  \n",
       "2      2023-01-30 18:49:28+00:00  \n",
       "3      2023-01-30 18:48:43+00:00  \n",
       "4      2023-01-30 18:48:26+00:00  \n",
       "5      2023-01-30 18:48:10+00:00  \n",
       "...                          ...  \n",
       "85215  2023-05-01 07:09:53+00:00  \n",
       "85221  2023-05-01 07:08:43+00:00  \n",
       "85223  2023-05-01 07:07:36+00:00  \n",
       "85224  2023-05-01 07:07:15+00:00  \n",
       "85225  2023-05-01 07:06:37+00:00  \n",
       "\n",
       "[61619 rows x 4 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Remove stop words'''\n",
    "    stops = set(stopwords.words('english'))\n",
    "    df['text'] = df['text'].apply(lambda x: [item for item in x if item not in stops])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = remove_stop_words(df)\n",
    "df = df[df.text.str.len() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comment</td>\n",
       "      <td>UNCwesRPh</td>\n",
       "      <td>[twitter, prior, musk, takeover, talking, dire...</td>\n",
       "      <td>2023-01-30 18:49:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment</td>\n",
       "      <td>wgp3</td>\n",
       "      <td>[article, say, imply, states, feature, turns, ...</td>\n",
       "      <td>2023-01-30 18:49:28+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comment</td>\n",
       "      <td>HighAndDrunk</td>\n",
       "      <td>[og, musk, duck, lives, wall]</td>\n",
       "      <td>2023-01-30 18:48:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Louismaxwell23</td>\n",
       "      <td>[dare, speak, way, great, powerful, musk, obvi...</td>\n",
       "      <td>2023-01-30 18:48:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Copykill</td>\n",
       "      <td>[cannot, wait, finally, excuse, shower, douche...</td>\n",
       "      <td>2023-01-30 18:48:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85215</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Emble12</td>\n",
       "      <td>[like, brain, dead, piranhas, lmao, anything, ...</td>\n",
       "      <td>2023-05-01 07:09:53+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85221</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Pizza_in_Space</td>\n",
       "      <td>[lying, agenda, please, correct, errors, anyth...</td>\n",
       "      <td>2023-05-01 07:08:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85223</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Da1realBigA</td>\n",
       "      <td>[hard, disagree, think, parody, elon, musk, su...</td>\n",
       "      <td>2023-05-01 07:07:36+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85224</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Viperions</td>\n",
       "      <td>[yeah, think, many, things, lining, right, sma...</td>\n",
       "      <td>2023-05-01 07:07:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85225</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Zealousideal_Plum498</td>\n",
       "      <td>[good, choice, musk, impossible, regulate, lev...</td>\n",
       "      <td>2023-05-01 07:06:37+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61282 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                author  \\\n",
       "0      Comment             UNCwesRPh   \n",
       "2      Comment                  wgp3   \n",
       "3      Comment          HighAndDrunk   \n",
       "4      Comment        Louismaxwell23   \n",
       "5      Comment              Copykill   \n",
       "...        ...                   ...   \n",
       "85215  Comment               Emble12   \n",
       "85221  Comment        Pizza_in_Space   \n",
       "85223  Comment           Da1realBigA   \n",
       "85224  Comment             Viperions   \n",
       "85225  Comment  Zealousideal_Plum498   \n",
       "\n",
       "                                                    text  \\\n",
       "0      [twitter, prior, musk, takeover, talking, dire...   \n",
       "2      [article, say, imply, states, feature, turns, ...   \n",
       "3                          [og, musk, duck, lives, wall]   \n",
       "4      [dare, speak, way, great, powerful, musk, obvi...   \n",
       "5      [cannot, wait, finally, excuse, shower, douche...   \n",
       "...                                                  ...   \n",
       "85215  [like, brain, dead, piranhas, lmao, anything, ...   \n",
       "85221  [lying, agenda, please, correct, errors, anyth...   \n",
       "85223  [hard, disagree, think, parody, elon, musk, su...   \n",
       "85224  [yeah, think, many, things, lining, right, sma...   \n",
       "85225  [good, choice, musk, impossible, regulate, lev...   \n",
       "\n",
       "                         created  \n",
       "0      2023-01-30 18:49:31+00:00  \n",
       "2      2023-01-30 18:49:28+00:00  \n",
       "3      2023-01-30 18:48:43+00:00  \n",
       "4      2023-01-30 18:48:26+00:00  \n",
       "5      2023-01-30 18:48:10+00:00  \n",
       "...                          ...  \n",
       "85215  2023-05-01 07:09:53+00:00  \n",
       "85221  2023-05-01 07:08:43+00:00  \n",
       "85223  2023-05-01 07:07:36+00:00  \n",
       "85224  2023-05-01 07:07:15+00:00  \n",
       "85225  2023-05-01 07:06:37+00:00  \n",
       "\n",
       "[61282 rows x 4 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:57:06: collecting all words and their counts\n",
      "INFO - 16:57:06: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 16:57:08: PROGRESS: at sentence #50000, processed 1986720 words and 1229758 word types\n",
      "INFO - 16:57:09: collected 1455363 token types (unigram + bigrams) from a corpus of 2450943 words and 61282 sentences\n",
      "INFO - 16:57:09: merged Phrases<1455363 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 16:57:09: Phrases lifecycle event {'msg': 'built Phrases<1455363 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000> in 2.14s', 'datetime': '2023-05-20T16:57:09.005975', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'created'}\n",
      "INFO - 16:57:09: exporting phrases from Phrases<1455363 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 16:57:10: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<114056 phrases, min_count=1, threshold=10.0> from Phrases<1455363 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000> in 1.77s', 'datetime': '2023-05-20T16:57:10.815766', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['article',\n",
       " 'say',\n",
       " 'imply',\n",
       " 'states',\n",
       " 'feature',\n",
       " 'turns',\n",
       " 'crashes',\n",
       " 'tesla',\n",
       " 'admits',\n",
       " 'default_behavior',\n",
       " 'never',\n",
       " 'tried_hide',\n",
       " 'hence',\n",
       " 'whole',\n",
       " 'counting',\n",
       " 'crash',\n",
       " 'disengaged',\n",
       " 'within',\n",
       " '5_seconds',\n",
       " 'crash',\n",
       " 'article',\n",
       " 'say',\n",
       " 'brings',\n",
       " 'question',\n",
       " 'times',\n",
       " 'musk',\n",
       " 'said',\n",
       " 'feature',\n",
       " 'give',\n",
       " 'proof',\n",
       " 'times',\n",
       " 'musk',\n",
       " 'said',\n",
       " 'fact',\n",
       " 'referring',\n",
       " 'crashes',\n",
       " 'turned',\n",
       " 'right',\n",
       " 'crash',\n",
       " 'speculation',\n",
       " 'best']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = [row for row in df.text]\n",
    "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "sentences[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can separate files here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:57:10: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2023-05-20T16:57:10.841099', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'created'}\n",
      "INFO - 16:57:10: collecting all words and their counts\n",
      "INFO - 16:57:10: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:57:12: PROGRESS: at sentence #50000, processed 1634433 words, keeping 173136 word types\n",
      "INFO - 16:57:12: collected 190207 word types from a corpus of 2016887 raw words and 61282 sentences\n",
      "INFO - 16:57:12: Creating a fresh vocabulary\n",
      "INFO - 16:57:12: Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 70489 unique words (37.06% of original 190207, drops 119718)', 'datetime': '2023-05-20T16:57:12.427662', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 16:57:12: Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 1840655 word corpus (91.26% of original 2016887, drops 176232)', 'datetime': '2023-05-20T16:57:12.428060', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 16:57:12: deleting the raw counts dictionary of 190207 items\n",
      "INFO - 16:57:12: sample=1e-05 downsamples 4493 most-common words\n",
      "INFO - 16:57:12: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 858112.1592917125 word corpus (46.6%% of prior 1840655)', 'datetime': '2023-05-20T16:57:12.605316', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 16:57:12: estimated required memory for 70489 words and 300 dimensions: 204418100 bytes\n",
      "INFO - 16:57:12: resetting layer weights\n",
      "INFO - 16:57:12: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-05-20T16:57:12.965795', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.04 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     vector_size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:57:12: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 70489 vocabulary and 300 features, using sg=0 hs=0 sample=1e-05 negative=20 window=4 shrink_windows=True', 'datetime': '2023-05-20T16:57:12.980528', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'train'}\n",
      "INFO - 16:57:14: EPOCH 0 - PROGRESS: at 32.84% examples, 276828 words/s, in_qsize 12, out_qsize 2\n",
      "INFO - 16:57:15: EPOCH 0 - PROGRESS: at 73.08% examples, 302822 words/s, in_qsize 11, out_qsize 2\n",
      "INFO - 16:57:15: EPOCH 0: training on 2016887 raw words (858260 effective words) took 2.6s, 325924 effective words/s\n",
      "INFO - 16:57:16: EPOCH 1 - PROGRESS: at 33.36% examples, 261017 words/s, in_qsize 5, out_qsize 9\n",
      "INFO - 16:57:17: EPOCH 1 - PROGRESS: at 75.05% examples, 299822 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 16:57:18: EPOCH 1: training on 2016887 raw words (858933 effective words) took 2.7s, 319732 effective words/s\n",
      "INFO - 16:57:19: EPOCH 2 - PROGRESS: at 34.73% examples, 287301 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:57:20: EPOCH 2 - PROGRESS: at 73.65% examples, 308219 words/s, in_qsize 10, out_qsize 2\n",
      "INFO - 16:57:21: EPOCH 2: training on 2016887 raw words (856924 effective words) took 2.7s, 323259 effective words/s\n",
      "INFO - 16:57:22: EPOCH 3 - PROGRESS: at 33.27% examples, 280507 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:57:23: EPOCH 3 - PROGRESS: at 71.60% examples, 301889 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:57:23: EPOCH 3: training on 2016887 raw words (858802 effective words) took 2.6s, 324393 effective words/s\n",
      "INFO - 16:57:24: EPOCH 4 - PROGRESS: at 33.78% examples, 251294 words/s, in_qsize 11, out_qsize 2\n",
      "INFO - 16:57:25: EPOCH 4 - PROGRESS: at 71.62% examples, 276705 words/s, in_qsize 10, out_qsize 5\n",
      "INFO - 16:57:26: EPOCH 4: training on 2016887 raw words (857988 effective words) took 2.8s, 304647 effective words/s\n",
      "INFO - 16:57:27: EPOCH 5 - PROGRESS: at 35.23% examples, 287732 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:57:28: EPOCH 5 - PROGRESS: at 74.10% examples, 303392 words/s, in_qsize 11, out_qsize 5\n",
      "INFO - 16:57:29: EPOCH 5: training on 2016887 raw words (858278 effective words) took 2.6s, 326313 effective words/s\n",
      "INFO - 16:57:30: EPOCH 6 - PROGRESS: at 35.23% examples, 301495 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:57:31: EPOCH 6 - PROGRESS: at 74.12% examples, 312939 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 16:57:31: EPOCH 6: training on 2016887 raw words (858295 effective words) took 2.6s, 326329 effective words/s\n",
      "INFO - 16:57:32: EPOCH 7 - PROGRESS: at 33.84% examples, 290544 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:57:33: EPOCH 7 - PROGRESS: at 73.08% examples, 305877 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:57:34: EPOCH 7: training on 2016887 raw words (858866 effective words) took 2.6s, 324124 effective words/s\n",
      "INFO - 16:57:35: EPOCH 8 - PROGRESS: at 35.23% examples, 302810 words/s, in_qsize 11, out_qsize 0\n",
      "INFO - 16:57:36: EPOCH 8 - PROGRESS: at 73.54% examples, 313343 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:57:37: EPOCH 8: training on 2016887 raw words (857656 effective words) took 2.6s, 327119 effective words/s\n",
      "INFO - 16:57:38: EPOCH 9 - PROGRESS: at 33.78% examples, 280623 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 16:57:39: EPOCH 9 - PROGRESS: at 73.10% examples, 306361 words/s, in_qsize 9, out_qsize 3\n",
      "INFO - 16:57:39: EPOCH 9: training on 2016887 raw words (857957 effective words) took 2.6s, 326449 effective words/s\n",
      "INFO - 16:57:40: EPOCH 10 - PROGRESS: at 33.41% examples, 286405 words/s, in_qsize 11, out_qsize 2\n",
      "INFO - 16:57:41: EPOCH 10 - PROGRESS: at 72.45% examples, 302084 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:57:42: EPOCH 10: training on 2016887 raw words (857457 effective words) took 2.6s, 323601 effective words/s\n",
      "INFO - 16:57:43: EPOCH 11 - PROGRESS: at 32.26% examples, 254768 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 16:57:44: EPOCH 11 - PROGRESS: at 68.26% examples, 273875 words/s, in_qsize 12, out_qsize 3\n",
      "INFO - 16:57:45: EPOCH 11: training on 2016887 raw words (858416 effective words) took 2.8s, 304351 effective words/s\n",
      "INFO - 16:57:46: EPOCH 12 - PROGRESS: at 32.31% examples, 277601 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:57:47: EPOCH 12 - PROGRESS: at 70.94% examples, 298826 words/s, in_qsize 12, out_qsize 3\n",
      "INFO - 16:57:47: EPOCH 12: training on 2016887 raw words (856990 effective words) took 2.6s, 325054 effective words/s\n",
      "INFO - 16:57:48: EPOCH 13 - PROGRESS: at 36.06% examples, 300757 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:57:50: EPOCH 13 - PROGRESS: at 74.05% examples, 307386 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:57:50: EPOCH 13: training on 2016887 raw words (857707 effective words) took 2.6s, 327549 effective words/s\n",
      "INFO - 16:57:51: EPOCH 14 - PROGRESS: at 33.30% examples, 287872 words/s, in_qsize 10, out_qsize 4\n",
      "INFO - 16:57:52: EPOCH 14 - PROGRESS: at 72.97% examples, 312404 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:57:53: EPOCH 14: training on 2016887 raw words (857749 effective words) took 2.6s, 327459 effective words/s\n",
      "INFO - 16:57:54: EPOCH 15 - PROGRESS: at 35.15% examples, 304505 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 16:57:55: EPOCH 15 - PROGRESS: at 72.97% examples, 311351 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:57:55: EPOCH 15: training on 2016887 raw words (858109 effective words) took 2.6s, 326340 effective words/s\n",
      "INFO - 16:57:56: EPOCH 16 - PROGRESS: at 35.15% examples, 293806 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:57:57: EPOCH 16 - PROGRESS: at 72.97% examples, 305194 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 16:57:58: EPOCH 16: training on 2016887 raw words (858183 effective words) took 2.6s, 327215 effective words/s\n",
      "INFO - 16:57:59: EPOCH 17 - PROGRESS: at 35.23% examples, 294945 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:00: EPOCH 17 - PROGRESS: at 75.91% examples, 318344 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 16:58:01: EPOCH 17: training on 2016887 raw words (858083 effective words) took 2.6s, 326982 effective words/s\n",
      "INFO - 16:58:02: EPOCH 18 - PROGRESS: at 34.71% examples, 292607 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:03: EPOCH 18 - PROGRESS: at 74.69% examples, 313888 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:58:03: EPOCH 18: training on 2016887 raw words (858467 effective words) took 2.8s, 308271 effective words/s\n",
      "INFO - 16:58:04: EPOCH 19 - PROGRESS: at 35.23% examples, 299367 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:06: EPOCH 19 - PROGRESS: at 72.59% examples, 305210 words/s, in_qsize 11, out_qsize 4\n",
      "INFO - 16:58:06: EPOCH 19: training on 2016887 raw words (857883 effective words) took 2.6s, 327028 effective words/s\n",
      "INFO - 16:58:07: EPOCH 20 - PROGRESS: at 33.84% examples, 285727 words/s, in_qsize 11, out_qsize 3\n",
      "INFO - 16:58:08: EPOCH 20 - PROGRESS: at 73.54% examples, 311163 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:58:09: EPOCH 20: training on 2016887 raw words (857445 effective words) took 2.6s, 325247 effective words/s\n",
      "INFO - 16:58:10: EPOCH 21 - PROGRESS: at 34.81% examples, 294880 words/s, in_qsize 13, out_qsize 1\n",
      "INFO - 16:58:11: EPOCH 21 - PROGRESS: at 71.54% examples, 303745 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:58:11: EPOCH 21: training on 2016887 raw words (857587 effective words) took 2.7s, 316347 effective words/s\n",
      "INFO - 16:58:13: EPOCH 22 - PROGRESS: at 35.63% examples, 297450 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:14: EPOCH 22 - PROGRESS: at 73.56% examples, 303680 words/s, in_qsize 13, out_qsize 1\n",
      "INFO - 16:58:14: EPOCH 22: training on 2016887 raw words (858055 effective words) took 2.6s, 323799 effective words/s\n",
      "INFO - 16:58:15: EPOCH 23 - PROGRESS: at 34.73% examples, 294950 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:58:16: EPOCH 23 - PROGRESS: at 73.02% examples, 304669 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:17: EPOCH 23: training on 2016887 raw words (858327 effective words) took 2.6s, 325637 effective words/s\n",
      "INFO - 16:58:18: EPOCH 24 - PROGRESS: at 34.30% examples, 283365 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 16:58:19: EPOCH 24 - PROGRESS: at 72.45% examples, 301915 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:19: EPOCH 24: training on 2016887 raw words (857905 effective words) took 2.6s, 325542 effective words/s\n",
      "INFO - 16:58:20: EPOCH 25 - PROGRESS: at 36.49% examples, 307095 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:22: EPOCH 25 - PROGRESS: at 75.55% examples, 314640 words/s, in_qsize 13, out_qsize 1\n",
      "INFO - 16:58:22: EPOCH 25: training on 2016887 raw words (857982 effective words) took 2.5s, 336870 effective words/s\n",
      "INFO - 16:58:23: EPOCH 26 - PROGRESS: at 35.64% examples, 300034 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:24: EPOCH 26 - PROGRESS: at 67.16% examples, 283530 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:25: EPOCH 26: training on 2016887 raw words (859226 effective words) took 2.8s, 304972 effective words/s\n",
      "INFO - 16:58:26: EPOCH 27 - PROGRESS: at 33.27% examples, 278479 words/s, in_qsize 12, out_qsize 2\n",
      "INFO - 16:58:27: EPOCH 27 - PROGRESS: at 73.08% examples, 302350 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:28: EPOCH 27: training on 2016887 raw words (858480 effective words) took 2.7s, 322431 effective words/s\n",
      "INFO - 16:58:29: EPOCH 28 - PROGRESS: at 33.32% examples, 285834 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:30: EPOCH 28 - PROGRESS: at 72.00% examples, 306297 words/s, in_qsize 11, out_qsize 1\n",
      "INFO - 16:58:30: EPOCH 28: training on 2016887 raw words (857651 effective words) took 2.7s, 317113 effective words/s\n",
      "INFO - 16:58:31: EPOCH 29 - PROGRESS: at 25.15% examples, 213907 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:32: EPOCH 29 - PROGRESS: at 58.10% examples, 243524 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 16:58:33: EPOCH 29 - PROGRESS: at 94.51% examples, 264302 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 16:58:33: EPOCH 29: training on 2016887 raw words (857802 effective words) took 3.2s, 271584 effective words/s\n",
      "INFO - 16:58:33: Word2Vec lifecycle event {'msg': 'training on 60506610 raw words (25741463 effective words) took 80.9s, 318108 effective words/s', 'datetime': '2023-05-20T16:58:33.931422', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'train'}\n",
      "/var/folders/dn/6362h3w12mqfb_jh4vg90vp40000gn/T/ipykernel_45371/1733578886.py:10: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n",
      "WARNING - 16:58:33: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 1.35 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "w2v_model.train(sentences,\n",
    "                total_examples=w2v_model.corpus_count,\n",
    "                epochs=30,\n",
    "                report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:58:33: Word2Vec lifecycle event {'fname_or_handle': 'word2vec.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-05-20T16:58:33.993721', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'saving'}\n",
      "INFO - 16:58:33: storing np array 'vectors' to word2vec.model.wv.vectors.npy\n",
      "INFO - 16:58:34: storing np array 'syn1neg' to word2vec.model.syn1neg.npy\n",
      "INFO - 16:58:34: not storing attribute cum_table\n",
      "INFO - 16:58:34: saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export = df.copy()\n",
    "file_export['old_text'] = file_export.text\n",
    "file_export.old_text = file_export.old_text.str.join(' ')\n",
    "file_export.text = file_export.text.apply(lambda x: ' '.join(bigram[x]))\n",
    "file_export.to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:58:36: loading Word2Vec object from word2vec.model\n",
      "INFO - 16:58:36: loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "INFO - 16:58:36: loading vectors from word2vec.model.wv.vectors.npy with mmap=None\n",
      "INFO - 16:58:36: loading syn1neg from word2vec.model.syn1neg.npy with mmap=None\n",
      "INFO - 16:58:36: setting ignored attribute cum_table to None\n",
      "INFO - 16:58:36: Word2Vec lifecycle event {'fname': 'word2vec.model', 'datetime': '2023-05-20T16:58:36.386321', 'gensim': '4.3.1', 'python': '3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:06:05) \\n[Clang 13.0.0 (clang-1300.0.29.3)]', 'platform': 'macOS-13.3-arm64-arm-64bit', 'event': 'loaded'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(max_iter=1000, n_clusters=2, n_init=50, random_state=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(max_iter=1000, n_clusters=2, n_init=50, random_state=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(max_iter=1000, n_clusters=2, n_init=50, random_state=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "word_vectors = Word2Vec.load(\"word2vec.model\").wv\n",
    "\n",
    "model = KMeans(n_clusters=2,\n",
    "               max_iter=1000,\n",
    "               random_state=True,\n",
    "               n_init=50)\n",
    "model.fit(X=word_vectors.vectors.astype('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('goalposts', 0.9980942010879517),\n",
       " ('summary_video', 0.9979171752929688),\n",
       " ('exact_conversation', 0.9972283840179443),\n",
       " ('level_scrutiny', 0.9970115423202515),\n",
       " ('monumentally_stupid', 0.9969017505645752),\n",
       " ('valid_argument', 0.9968544244766235),\n",
       " ('tim_address', 0.9968184232711792),\n",
       " ('true_altruism', 0.996793806552887),\n",
       " ('massively_overpriced', 0.9967292547225952),\n",
       " ('revenue_generation', 0.996704638004303)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_vector(model.cluster_centers_[1], topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cluster_index = 1\n",
    "positive_cluster_center = model.cluster_centers_[positive_cluster_index]\n",
    "negative_cluster_center = model.cluster_centers_[1-positive_cluster_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_vectors.index_to_key)\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])\n",
    "words['cluster_value'] = [1 if i==positive_cluster_index else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>musk</td>\n",
       "      <td>[-0.07508382, -0.031636886, 0.017578892, -0.02...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978675</td>\n",
       "      <td>0.978675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>[-0.050290406, 0.028971367, 0.05361907, 0.0202...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.841271</td>\n",
       "      <td>-0.841271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elon_musk</td>\n",
       "      <td>[0.03359735, 0.004619063, 0.0552491, -0.006509...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.081044</td>\n",
       "      <td>1.081044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people</td>\n",
       "      <td>[-0.017200358, 0.018315213, -0.0030343845, 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.355219</td>\n",
       "      <td>1.355219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>like</td>\n",
       "      <td>[-0.11874396, -0.043377627, 0.00095364836, 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.305437</td>\n",
       "      <td>1.305437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70484</th>\n",
       "      <td>filth_water</td>\n",
       "      <td>[-0.09895438, -0.023136625, 0.02333415, 0.0712...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.118367</td>\n",
       "      <td>-3.118367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70485</th>\n",
       "      <td>humiliation_anal</td>\n",
       "      <td>[-0.09619634, -0.028401533, 0.020558404, 0.064...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.367644</td>\n",
       "      <td>-2.367644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70486</th>\n",
       "      <td>alpha_female</td>\n",
       "      <td>[-0.0903291, -0.020118501, 0.035740037, 0.0691...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.945927</td>\n",
       "      <td>-2.945927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70487</th>\n",
       "      <td>degraded_compared</td>\n",
       "      <td>[-0.08731098, -0.015690053, 0.037888546, 0.071...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.417990</td>\n",
       "      <td>-3.417990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70488</th>\n",
       "      <td>=_loss</td>\n",
       "      <td>[-0.043085422, 0.019017791, 0.05534655, 0.0511...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.602491</td>\n",
       "      <td>2.602491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70489 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   words                                            vectors  \\\n",
       "0                   musk  [-0.07508382, -0.031636886, 0.017578892, -0.02...   \n",
       "1                      -  [-0.050290406, 0.028971367, 0.05361907, 0.0202...   \n",
       "2              elon_musk  [0.03359735, 0.004619063, 0.0552491, -0.006509...   \n",
       "3                 people  [-0.017200358, 0.018315213, -0.0030343845, 0.0...   \n",
       "4                   like  [-0.11874396, -0.043377627, 0.00095364836, 0.0...   \n",
       "...                  ...                                                ...   \n",
       "70484        filth_water  [-0.09895438, -0.023136625, 0.02333415, 0.0712...   \n",
       "70485   humiliation_anal  [-0.09619634, -0.028401533, 0.020558404, 0.064...   \n",
       "70486       alpha_female  [-0.0903291, -0.020118501, 0.035740037, 0.0691...   \n",
       "70487  degraded_compared  [-0.08731098, -0.015690053, 0.037888546, 0.071...   \n",
       "70488             =_loss  [-0.043085422, 0.019017791, 0.05534655, 0.0511...   \n",
       "\n",
       "       cluster  cluster_value  closeness_score  sentiment_coeff  \n",
       "0            1              1         0.978675         0.978675  \n",
       "1            0             -1         0.841271        -0.841271  \n",
       "2            1              1         1.081044         1.081044  \n",
       "3            1              1         1.355219         1.355219  \n",
       "4            1              1         1.305437         1.305437  \n",
       "...        ...            ...              ...              ...  \n",
       "70484        0             -1         3.118367        -3.118367  \n",
       "70485        0             -1         2.367644        -2.367644  \n",
       "70486        0             -1         2.945927        -2.945927  \n",
       "70487        0             -1         3.417990        -3.417990  \n",
       "70488        1              1         2.602491         2.602491  \n",
       "\n",
       "[70489 rows x 6 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_not_in_dictionary(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Remove stop words'''\n",
    "    stops = set(words.words)\n",
    "    df['text'] = df['text'].apply(lambda x: [item for item in x if item in stops])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/6362h3w12mqfb_jh4vg90vp40000gn/T/ipykernel_45371/2479509448.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: [item for item in x if item in stops])\n"
     ]
    }
   ],
   "source": [
    "df = remove_not_in_dictionary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export = df.copy()\n",
    "file_export['old_text'] = file_export.text\n",
    "file_export.old_text = file_export.old_text.str.join(' ')\n",
    "file_export.text = file_export.text.apply(lambda x: ' '.join(bigram[x]))\n",
    "file_export.to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comment</td>\n",
       "      <td>UNCwesRPh</td>\n",
       "      <td>[twitter, prior, musk, takeover, talking, dire...</td>\n",
       "      <td>2023-01-30 18:49:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comment</td>\n",
       "      <td>wgp3</td>\n",
       "      <td>[article, say, imply, states, feature, turns, ...</td>\n",
       "      <td>2023-01-30 18:49:28+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comment</td>\n",
       "      <td>HighAndDrunk</td>\n",
       "      <td>[og, musk, duck, lives, wall]</td>\n",
       "      <td>2023-01-30 18:48:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Louismaxwell23</td>\n",
       "      <td>[dare, speak, way, great, powerful, musk, obvi...</td>\n",
       "      <td>2023-01-30 18:48:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Copykill</td>\n",
       "      <td>[cannot, wait, finally, excuse, shower, douche]</td>\n",
       "      <td>2023-01-30 18:48:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85215</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Emble12</td>\n",
       "      <td>[like, brain, dead, lmao, anything, barely, re...</td>\n",
       "      <td>2023-05-01 07:09:53+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85221</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Pizza_in_Space</td>\n",
       "      <td>[lying, agenda, please, correct, errors, anyth...</td>\n",
       "      <td>2023-05-01 07:08:43+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85223</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Da1realBigA</td>\n",
       "      <td>[hard, disagree, think, parody, elon, musk, su...</td>\n",
       "      <td>2023-05-01 07:07:36+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85224</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Viperions</td>\n",
       "      <td>[yeah, think, many, things, lining, right, sma...</td>\n",
       "      <td>2023-05-01 07:07:15+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85225</th>\n",
       "      <td>Comment</td>\n",
       "      <td>Zealousideal_Plum498</td>\n",
       "      <td>[good, choice, musk, impossible, regulate, lev...</td>\n",
       "      <td>2023-05-01 07:06:37+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61282 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                author  \\\n",
       "0      Comment             UNCwesRPh   \n",
       "2      Comment                  wgp3   \n",
       "3      Comment          HighAndDrunk   \n",
       "4      Comment        Louismaxwell23   \n",
       "5      Comment              Copykill   \n",
       "...        ...                   ...   \n",
       "85215  Comment               Emble12   \n",
       "85221  Comment        Pizza_in_Space   \n",
       "85223  Comment           Da1realBigA   \n",
       "85224  Comment             Viperions   \n",
       "85225  Comment  Zealousideal_Plum498   \n",
       "\n",
       "                                                    text  \\\n",
       "0      [twitter, prior, musk, takeover, talking, dire...   \n",
       "2      [article, say, imply, states, feature, turns, ...   \n",
       "3                          [og, musk, duck, lives, wall]   \n",
       "4      [dare, speak, way, great, powerful, musk, obvi...   \n",
       "5        [cannot, wait, finally, excuse, shower, douche]   \n",
       "...                                                  ...   \n",
       "85215  [like, brain, dead, lmao, anything, barely, re...   \n",
       "85221  [lying, agenda, please, correct, errors, anyth...   \n",
       "85223  [hard, disagree, think, parody, elon, musk, su...   \n",
       "85224  [yeah, think, many, things, lining, right, sma...   \n",
       "85225  [good, choice, musk, impossible, regulate, lev...   \n",
       "\n",
       "                         created  \n",
       "0      2023-01-30 18:49:31+00:00  \n",
       "2      2023-01-30 18:49:28+00:00  \n",
       "3      2023-01-30 18:48:43+00:00  \n",
       "4      2023-01-30 18:48:26+00:00  \n",
       "5      2023-01-30 18:48:10+00:00  \n",
       "...                          ...  \n",
       "85215  2023-05-01 07:09:53+00:00  \n",
       "85221  2023-05-01 07:08:43+00:00  \n",
       "85223  2023-05-01 07:07:36+00:00  \n",
       "85224  2023-05-01 07:07:15+00:00  \n",
       "85225  2023-05-01 07:06:37+00:00  \n",
       "\n",
       "[61282 rows x 4 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
